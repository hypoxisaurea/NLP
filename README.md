# NLP Study

### word2vec
단어 벡터 간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화 할 수 있는 방법

### doc2vec
word2vec을 변형하여 문서의 임베딩을 얻을 수 있도록 한 알고리즘

### seq2seq
입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 모델<br>
인코더-디코더라는 두 개의 모듈로 구성

### attention
디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 재참고<br>
단, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분에 집중(attention)

### transformer
기존 seq2seq의 구조인 인코더-디코더를 따르면서도, 어텐션(Attention)만으로 구현한 모델<br>
RNN을 사용하지 않고, 인코더-디코더 구조를 설계하였음에도 번역 성능에서도 RNN보다 성능 우수

### BERT
트랜스포머를 이용하여 구현한, 레이블이 없는 텍스트 데이터로 사전 훈련된 언어 모델



<br><br><br>
\* 참고자료: https://wikidocs.net/book/2155
